{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c28cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfd9169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58872a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(ngram_range=(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b9e7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB  # CategoricalNB not compatible with sparse matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92b3a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2858eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.read_csv('./train.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e087f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "database.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "815de3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224125, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cf368bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, dt = train_test_split(database, train_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3990846d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22412, 14)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2294416f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(ngram_range=(1, 5))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(df['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "041c75f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 705141\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "#print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4cd7f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22412, 705141)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = vect.transform(df['comment'])\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45fafea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201713, 705141)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = vect.transform(dt['comment'])\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "945352e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price_value', 'fake_originality', 'warranty', 'size', 'discrepancy',\n",
       "       'flavor_odor', 'expiration_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "898399c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_train = pd.DataFrame()\n",
    "model_BernoulliNB = {}\n",
    "model_MultinomialNB = {}\n",
    "model_LogReg = {}\n",
    "for col in df.columns[7:]:\n",
    "    vec_train[col] = df[col].astype('bool')\n",
    "    model_BernoulliNB[col] = BernoulliNB().fit(X_train, vec_train[col])\n",
    "    model_MultinomialNB[col] = MultinomialNB().fit(X_train, vec_train[col])\n",
    "    model_LogReg[col] = LogisticRegression().fit(X_train, vec_train[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "455fdefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('price_value', 0.342, 0.962, 0.981)\n",
      "('fake_originality', 0.0, 0.874, 0.956)\n",
      "('warranty', 0.0, 0.892, 0.974)\n",
      "('size', 0.136, 0.971, 0.988)\n",
      "('discrepancy', 0.069, 0.977, 0.995)\n",
      "('flavor_odor', 0.044, 0.943, 0.988)\n",
      "('expiration_date', 0.0, 0.887, 0.975)\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for key, Bmod, Mmod, Lmod in zip(model_BernoulliNB.keys(),\n",
    "                           model_BernoulliNB.values(),\n",
    "                           model_MultinomialNB.values(),\n",
    "                           model_LogReg.values()):\n",
    "    \n",
    "    pred.append( (key,\n",
    "                  f1_score(Bmod.predict(X_train), vec_train[key]).round(3),\n",
    "                  f1_score(Mmod.predict(X_train), vec_train[key]).round(3),\n",
    "                  f1_score(Lmod.predict(X_train), vec_train[key]).round(3),) )\n",
    "print(*pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5cee015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('price_value', 0.006, 0.663, 0.807)\n",
      "('fake_originality', 0.0, 0.013, 0.19)\n",
      "('warranty', 0.0, 0.004, 0.145)\n",
      "('size', 0.001, 0.181, 0.652)\n",
      "('discrepancy', 0.0, 0.142, 0.565)\n",
      "('flavor_odor', 0.002, 0.317, 0.79)\n",
      "('expiration_date', 0.0, 0.027, 0.355)\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "vec_test = {}\n",
    "for key, Bmod, Mmod, Lmod in zip(model_BernoulliNB.keys(),\n",
    "                           model_BernoulliNB.values(),\n",
    "                           model_MultinomialNB.values(),\n",
    "                           model_LogReg.values()):\n",
    "    vec_test[key] = dt[key].astype('bool')\n",
    "    pred.append( (key,\n",
    "                  f1_score(Bmod.predict(X_test), vec_test[key]).round(3),\n",
    "                  f1_score(Mmod.predict(X_test), vec_test[key]).round(3),\n",
    "                  f1_score(Lmod.predict(X_test), vec_test[key]).round(3),) )\n",
    "print(*pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8847f2e",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "    <font size=4>\n",
    "    نکته اساسی در استفاده از مدل های نایو بیز استقلال متغیرها از یکدیگر است بر این اساس در زمان کار با متن نیاز است که این نکته به دقت بررسی شود. زیرا که به صورت کلی و ذاتی کلمات در زبان انسان از هم مستقل نیستند و در ارتباط با یکدیگر معنا می سازند و بخصوص این نکته در فعل های مثبت و منفی خود را نشان می دهند. در نتیجه هرچی موضوع ما به ماهیت معنایی جملات ارتباط بیشتری داشته باشد اعتبار مدل های نیو بیز کمتر می شود. در بررسی بالا هم مشاهده می شود که هر چه موضوع کلی تر بوده دقت مدل های نایو بیز هم بالاتر رفته اما استفاده از مدل لاجستیک ریگریشن با توجه به بررسی اثر تقابل متغیرها در تمامی حالات بهتر عمل کرده اما هرچه اثر معنایی جملات در برچسب ها بالاتر رفته، این مدل نیز که به دنبال یک رابطه خطی هست ضعیف تر عملکرده و در واقع نیاز به بررسی های نحوی متن از یک سو بیشتر مورد لزوم به نظر می رسد و از سوی دیگر نیاز به مدل های غیر خطی برای تفکیک بهتر ارتباط معنایی کلمات با هم  \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8c269",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "    <font size=4>\n",
    "        جدای از این مسئله مفهومی، اعداد بالا که سری اول دقت مدل ها روی داده های تعلیم و سری دوم دقت روی داده های تست با این نکته که بخاطر حجم بالا در این جا 10 درصد داده ها برای تعلیم استفاده شده است. نشان می دهد که مدل نایو بیز در اینجا یا اصلن دارای underfitting هست و یا که شدیدن دچار overfitting زیرا که مستقل از دقت مدل در داده های تعلیم، دقت افت شدیدی در داده های تست متحمل شده است.  \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "015e2e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f07e221",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "    <font size=4>\n",
    "        با اضافه کردن دسته بندی محصولات به کامنت ها و استفاده از درخت های تصمیم گیری به خاطر سبک تر شدن محاسبات نسبت به لاجستیک ریگریشن عملکرد بهتری در ادامه کسب شده است.  \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe08a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "database.category_id = database.category_id.astype('category').cat.codes\n",
    "database.product_id = database.product_id.astype('category').cat.codes\n",
    "database.is_buyer = database.is_buyer == 'True'\n",
    "for col in database.columns[7:]:\n",
    "    database[col] = database[col].astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f4ecb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "543b3454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "\tcol               \ttrain score       \ttest score       \n",
      "\tprice_value       \t0.691             \t0.661            \n",
      "\tfake_originality  \t0.06              \t0.071            \n",
      "\twarranty          \t0.052             \t0.057            \n",
      "\tsize              \t0.469             \t0.409            \n",
      "\tdiscrepancy       \t0.315             \t0.307            \n",
      "\tflavor_odor       \t0.7               \t0.649            \n",
      "\texpiration_date   \t0.018             \t0.024            \n",
      "(1, 2)\n",
      "\tcol               \ttrain score       \ttest score       \n",
      "\tprice_value       \t0.592             \t0.508            \n",
      "\tfake_originality  \t0.048             \t0.03             \n",
      "\twarranty          \t0.053             \t0.012            \n",
      "\tsize              \t0.303             \t0.19             \n",
      "\tdiscrepancy       \t0.231             \t0.191            \n",
      "\tflavor_odor       \t0.191             \t0.063            \n",
      "\texpiration_date   \t0.005             \t0.017            \n",
      "(1, 3)\n",
      "\tcol               \ttrain score       \ttest score       \n",
      "\tprice_value       \t0.514             \t0.363            \n",
      "\tfake_originality  \t0.046             \t0.014            \n",
      "\twarranty          \t0.059             \t0.011            \n",
      "\tsize              \t0.242             \t0.086            \n",
      "\tdiscrepancy       \t0.192             \t0.11             \n",
      "\tflavor_odor       \t0.109             \t0.023            \n",
      "\texpiration_date   \t0.005             \t0.0              \n",
      "(1, 4)\n",
      "\tcol               \ttrain score       \ttest score       \n",
      "\tprice_value       \t0.475             \t0.267            \n",
      "\tfake_originality  \t0.049             \t0.004            \n",
      "\twarranty          \t0.067             \t0.014            \n",
      "\tsize              \t0.225             \t0.053            \n",
      "\tdiscrepancy       \t0.181             \t0.073            \n",
      "\tflavor_odor       \t0.092             \t0.017            \n",
      "\texpiration_date   \t0.005             \t0.0              \n",
      "(1, 5)\n",
      "\tcol               \ttrain score       \ttest score       \n",
      "\tprice_value       \t0.456             \t0.204            \n",
      "\tfake_originality  \t0.052             \t0.005            \n",
      "\twarranty          \t0.069             \t0.014            \n",
      "\tsize              \t0.22              \t0.036            \n",
      "\tdiscrepancy       \t0.179             \t0.057            \n",
      "\tflavor_odor       \t0.088             \t0.013            \n",
      "\texpiration_date   \t0.004             \t0.0              \n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "for ngram_ranges in [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5)]:\n",
    "    model_scores = [('col', 'train score', 'test score')]\n",
    "    print(ngram_ranges)\n",
    "    print(*(f'\\t{txt:17}' for txt in model_scores[-1]))\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_ranges)\n",
    "    X = sparse.hstack([vectorizer.fit_transform(train.comment), train[['category_id']].astype('category')])\n",
    "    X_t = sparse.hstack([vectorizer.transform(test.comment), test[['category_id']].astype('category')])\n",
    "    for col in df.columns[7:]:\n",
    "        model_M = MultinomialNB().fit(X, train[col])\n",
    "        model_scores.append((col,\n",
    "                            f1_score(train[col], model_M.predict(X)).round(3),\n",
    "                            f1_score(test[col], model_M.predict(X_t)).round(3),\n",
    "                            ))\n",
    "        print(*(f'\\t{str(txt):17}' for txt in model_scores[-1]))\n",
    "    models[ngram_ranges] = model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26008cc0",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "    <font size=4>\n",
    "        در بالا مشاهده میشه که با افزایش ان-گرام ها در مرحله اول برای ایجاد ترکیبات معادله معانی جملات این کار در روش نایو بیز خوب عمل کرده و باعث افت شدید نتیجه ها شده است.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "177fdfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e29502",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "    <font size=4>\n",
    "        برای بررسی ساده و کاهش ابعاد مسئله ابتدا سراغ ستون بو و مزه رفته و با بررسی کامنت های مثبت و ایجاد یک دیکشنری دستی سبک به عملکرد درخت تصمیم گیری پرداخته شده است.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "013a71c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flavor_odor\n",
      "\t#train:168093  #test:56032\n",
      "\t[per recall] train: [0.995 0.909]\n",
      "\t[per recall] test: [0.993 0.87 ]\n",
      "\tf1 train: 0.909\n",
      "\tf1 test: 0.87\n"
     ]
    }
   ],
   "source": [
    "words = set(['خوش', 'خوشمزه', 'خوش مزه', 'خوشمزس', 'بو', 'بوی', 'مزه', 'مزی', 'بوش', 'عطر', 'معطر', 'خوشبو', 'رایحه',\n",
    "         'طعم', 'طعمش', 'خوشمزه', 'رایحه', 'شیرین', 'تند', 'شور', 'تلخ', 'مزس', 'خوشمزست', 'شیرینه', 'طعمه', 'اسانس', 'بوشم'\n",
    "         , 'خوشمزههه', 'خوشبوء', 'طعمه', 'تنده'])\n",
    "subject = 'flavor_odor'\n",
    "print(subject)\n",
    "vectorizer = CountVectorizer(binary=True, vocabulary=dict(zip(words,range(len(words)))))\n",
    "X = sparse.hstack([vectorizer.transform(train.comment), train[['category_id']].astype('category')])\n",
    "X_t = sparse.hstack([vectorizer.transform(test.comment), test[['category_id']].astype('category')])\n",
    "price_value_Mul_all = DecisionTreeClassifier().fit(X, train[subject])\n",
    "print(f'\\t#train:{train.shape[0]}  #test:{test.shape[0]}')\n",
    "print('\\t[per recall] train:', f1_score(train[subject], price_value_Mul_all.predict(X), average=None).round(3))\n",
    "print('\\t[per recall] test:', f1_score(test[subject], price_value_Mul_all.predict(X_t), average=None).round(3))\n",
    "print('\\tf1 train:', f1_score(train[subject], price_value_Mul_all.predict(X)).round(3))\n",
    "print('\\tf1 test:', f1_score(test[subject], price_value_Mul_all.predict(X_t)).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d78d5",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "    <font size=4>\n",
    "        با یک دیکشنری کوچک و محاسبات سبک مشاهده می شود که این مدل با دقت بسیار خوبی در باره این ستون تصمیم گیری کرده است.  اما خوب ساختن دیکشنری به صورت دستی عملن در یادگیری ماشین کار صحیحی نیست و یک ایده برای سبک کردن این دیکشنری و کاهش محاسبات لازم است. \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b3fff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import filterfalse\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b737301",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "    <font size=4>\n",
    "        \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d179734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t#train:168093  #test:56032\n",
      "\t[per recall] train: [0. 0. 0. 0. 0. 0. 0.]\n",
      "\t[per recall] test: [0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "ngram_range_set = (1, 2)\n",
    "vectorizer = CountVectorizer(binary=True, ngram_range=ngram_range_set).fit(train.comment)\n",
    "X = sparse.hstack([vectorizer.transform(train.comment), train[['category_id']].astype('category')])\n",
    "X_t = sparse.hstack([vectorizer.transform(test.comment), test[['category_id']].astype('category')])\n",
    "Mul_all = RandomForestClassifier(n_estimators=9,\n",
    "                                 max_depth=7,\n",
    "                                 min_samples_split=0.25,\n",
    "                                 min_samples_leaf=10,\n",
    "                                 n_jobs=-1,\n",
    "                                ).fit(X, train.iloc[:, -7:])\n",
    "print(f'\\t#train:{train.shape[0]}  #test:{test.shape[0]}')\n",
    "print('\\t[per recall] train:', f1_score(train.iloc[:, -7:], Mul_all.predict(X), average=None).round(3))\n",
    "print('\\t[per recall] test:', f1_score(test.iloc[:, -7:], Mul_all.predict(X_t), average=None).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac54f75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flavor_odor\n"
     ]
    }
   ],
   "source": [
    "subject = 'flavor_odor'\n",
    "print(subject)\n",
    "ngram_range_set = (2, 7)\n",
    "price_value_vc_p = CountVectorizer(binary=True, ngram_range=ngram_range_set).fit(train.loc[train[subject]==1.0, 'comment'])\n",
    "price_value_vc_n = CountVectorizer(binary=True, ngram_range=ngram_range_set).fit(train.loc[train[subject]==0.0, 'comment'])\n",
    "my_dict = {ngram:valv for valv, ngram in enumerate(\n",
    "    filterfalse(lambda ngram: ngram in price_value_vc_n.vocabulary_.keys(),\n",
    "               price_value_vc_p.vocabulary_.keys()))}\n",
    "vectorizer = CountVectorizer(binary=True, ngram_range=ngram_range_set, vocabulary=my_dict)\n",
    "X = sparse.hstack([vectorizer.transform(train.comment), train[['category_id']].astype('category')])\n",
    "X_t = sparse.hstack([vectorizer.transform(test.comment), test[['category_id']].astype('category')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "51ce141d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t#train:168093  #test:56032\n",
      "\t[per recall] train: [0.995 0.907]\n",
      "\t[per recall] test: [0.98  0.409]\n",
      "\tf1 train: 0.907\n",
      "\tf1 test: 0.409\n"
     ]
    }
   ],
   "source": [
    "Mul_all = RandomForestClassifier(n_estimators=13,\n",
    "                                 n_jobs=-1,\n",
    "                                ).fit(X, train[subject])\n",
    "print(f'\\t#train:{train.shape[0]}  #test:{test.shape[0]}')\n",
    "print('\\t[per recall] train:', f1_score(train[subject], Mul_all.predict(X), average=None).round(3))\n",
    "print('\\t[per recall] test:', f1_score(test[subject], Mul_all.predict(X_t), average=None).round(3))\n",
    "print('\\tf1 train:', f1_score(train[subject], Mul_all.predict(X)).round(3))\n",
    "print('\\tf1 test:', f1_score(test[subject], Mul_all.predict(X_t)).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3aad3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t#train:168093  #test:56032\n",
      "\t[per recall] train: [0.999 0.978]\n",
      "\t[per recall] test: [0.989 0.749]\n",
      "\tf1 train: 0.978\n",
      "\tf1 test: 0.749\n"
     ]
    }
   ],
   "source": [
    "X = sparse.hstack([price_value_vc_p.transform(train.comment), train[['category_id']].astype('category')])\n",
    "X_t = sparse.hstack([price_value_vc_p.transform(test.comment), test[['category_id']].astype('category')])\n",
    "Mul_all = RandomForestClassifier(n_estimators=13,\n",
    "                                 n_jobs=-1,\n",
    "                                ).fit(X, train[subject])\n",
    "print(f'\\t#train:{train.shape[0]}  #test:{test.shape[0]}')\n",
    "print('\\t[per recall] train:', f1_score(train[subject], Mul_all.predict(X), average=None).round(3))\n",
    "print('\\t[per recall] test:', f1_score(test[subject], Mul_all.predict(X_t), average=None).round(3))\n",
    "print('\\tf1 train:', f1_score(train[subject], Mul_all.predict(X)).round(3))\n",
    "print('\\tf1 test:', f1_score(test[subject], Mul_all.predict(X_t)).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8be92e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_value\n",
      "\t[per recall] train: [0.996 0.983]\n",
      "\t[per recall] test: [0.946 0.745]\n",
      "\tf1 train: 0.983\n",
      "\tf1 test: 0.745\n",
      "fake_originality\n",
      "\t[per recall] train: [1.    0.948]\n",
      "\t[per recall] test: [0.997 0.269]\n",
      "\tf1 train: 0.948\n",
      "\tf1 test: 0.269\n",
      "warranty\n",
      "\t[per recall] train: [1.    0.935]\n",
      "\t[per recall] test: [0.999 0.097]\n",
      "\tf1 train: 0.935\n",
      "\tf1 test: 0.097\n",
      "size\n",
      "\t[per recall] train: [0.997 0.97 ]\n",
      "\t[per recall] test: [0.972 0.531]\n",
      "\tf1 train: 0.97\n",
      "\tf1 test: 0.531\n",
      "discrepancy\n",
      "\t[per recall] train: [0.998 0.961]\n",
      "\t[per recall] test: [0.981 0.273]\n",
      "\tf1 train: 0.961\n",
      "\tf1 test: 0.273\n",
      "flavor_odor\n",
      "\t[per recall] train: [0.999 0.976]\n",
      "\t[per recall] test: [0.988 0.736]\n",
      "\tf1 train: 0.976\n",
      "\tf1 test: 0.736\n",
      "expiration_date\n",
      "\t[per recall] train: [1.    0.957]\n",
      "\t[per recall] test: [0.999 0.373]\n",
      "\tf1 train: 0.957\n",
      "\tf1 test: 0.373\n"
     ]
    }
   ],
   "source": [
    "ngram_range_set = (2, 7)\n",
    "for subject in train.columns[-7:]:\n",
    "    print(subject)\n",
    "    vectorizer = CountVectorizer(binary=True, ngram_range=ngram_range_set).fit(train.loc[train[subject]==1.0, 'comment'])\n",
    "    X = sparse.hstack([vectorizer.transform(train.comment), train[['category_id']].astype('category')])\n",
    "    X_t = sparse.hstack([vectorizer.transform(test.comment), test[['category_id']].astype('category')])\n",
    "    Mul_all = RandomForestClassifier(n_estimators=13,\n",
    "                                 n_jobs=-1,\n",
    "                                ).fit(X, train[subject])\n",
    "    print('\\t[per recall] train:', f1_score(train[subject], Mul_all.predict(X), average=None).round(3))\n",
    "    print('\\t[per recall] test:', f1_score(test[subject], Mul_all.predict(X_t), average=None).round(3))\n",
    "    print('\\tf1 train:', f1_score(train[subject], Mul_all.predict(X)).round(3))\n",
    "    print('\\tf1 test:', f1_score(test[subject], Mul_all.predict(X_t)).round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
